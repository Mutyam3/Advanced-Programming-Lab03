{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 03: TensorFlow vs. PyTorch\n",
        "\n",
        "• Build and train the same small neural network model using both TensorFlow and PyTorch.\n",
        "\n",
        "• Measure and compare training time and performance.\n",
        "\n",
        "• Convert the trained models into lightweight formats suitable for embedded deployment: Tensor-\n",
        "Flow Lite and ONNX."
      ],
      "metadata": {
        "id": "JEyqtS1lBQrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Implementation"
      ],
      "metadata": {
        "id": "abqh1hCb81PN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJVqGvALPjXM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Use the MNIST dataset of handwritten digits (28x28 grayscale images, 10 classes).\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "3hDjFvIpQytI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab177b7-cfa9-4c72-864c-9f3ceaf7f53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture\n",
        "# • Flatten input (784 features)\n",
        "# • Dense layer with 64 ReLU units\n",
        "# • Output layer with 10 units (softmax for TensorFlow)"
      ],
      "metadata": {
        "id": "ipLnFtf-ShlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),              # Input shape: 28x28 pixels\n",
        "    tf.keras.layers.Flatten(),                          # Flatten to 784 features\n",
        "    tf.keras.layers.Dense(64, activation='relu'),       # Hidden layer with 64 neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Output layer: 10 classes (digits 0–9)\n",
        "])\n"
      ],
      "metadata": {
        "id": "5XDmpgtrRWo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi2ZVkYCScxu",
        "outputId": "e7900eeb-66f8-402a-878c-8ad071da0ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0580\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0487\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0428\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0343\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0296\n",
            "TF Training time: 24.67 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tensorflow - Test accuracy and Inference time using TensorFlow’s model.evaluate()"
      ],
      "metadata": {
        "id": "ixpJKIknATyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time and get evaluation metrics\n",
        "start_inference = time.time()\n",
        "\n",
        "# model.evaluate runs inference on the test set and returns [loss, accuracy]\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "end_inference = time.time()\n",
        "inference_time = end_inference - start_inference\n",
        "\n",
        "# Print results\n",
        "print(f\"Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Accuracy in Percentage: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Inference Time on Test Set: {inference_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnmPPVMWflkC",
        "outputId": "2c13f698-c86b-4eba-b129-df9fafbd39ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9721 - loss: 0.1013\n",
            "Loss: 0.0883440151810646\n",
            "Test Accuracy: 0.9753000140190125\n",
            "Test Accuracy in Percentage: 97.53%\n",
            "Inference Time on Test Set: 3.23 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the trained model to TensorFlow Lite using TFLiteConverter."
      ],
      "metadata": {
        "id": "Z4pRF0Nq_8J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LasW_2bsgm38",
        "outputId": "336b2946-1cce-4ac5-a9a9-55b6908ac82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpj14hcc2t'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_8')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134537999432016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134537999424144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134537999428944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134537999423760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Implementation"
      ],
      "metadata": {
        "id": "jXhzY84JnYQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    #  input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    #  input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(pred.data, 1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}:  Accuracy = {epoch_acc} - Loss = {epoch_loss}\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajF0rmvxK_J",
        "outputId": "eacdce3c-8eab-446d-cce2-0561d304e743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:  Accuracy = 90.64666666666666 - Loss = 0.34568143301258486\n",
            "Epoch 2:  Accuracy = 94.98333333333333 - Loss = 0.17169813975815973\n",
            "Epoch 3:  Accuracy = 96.34833333333333 - Loss = 0.12494718434549867\n",
            "Epoch 4:  Accuracy = 97.17166666666667 - Loss = 0.09756645267345011\n",
            "Epoch 5:  Accuracy = 97.69166666666666 - Loss = 0.07907850412273158\n",
            "PyTorch Training time: 49.84 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference and Evaluation using PyTorch’s model.eval() + torch.no_grad()."
      ],
      "metadata": {
        "id": "AqvUZyvEDF_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference and Evaluation using PyTorch’s model.eval() + torch.no_grad().\n",
        "model.eval()\n",
        "correct = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")\n",
        "print(f\"Test accuracy: {(correct / len(test_loader.dataset))*100:.2f}%\")\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4u-kq-62AHx",
        "outputId": "2476782d-4a11-44a8-f7de-2ccb5b7a6b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9671\n",
            "Test accuracy: 96.71%\n",
            "Inference Time: 0.9791 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert the trained model into lightweight formats - using ONNX\n",
        "1. Export the model to ONNX format.\n",
        "2. Use dummy input with correct shape (e.g. torch.randn(1, 784)).\n",
        "3. Save as model.onnx."
      ],
      "metadata": {
        "id": "EqDR7ghhplTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg8B05z4zBoi",
        "outputId": "ab16e1c3-4326-4c27-8db2-66e87e34b927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])\n",
        "print('Successfully saved the model as model.onnx')"
      ],
      "metadata": {
        "id": "8aM4mLT5zGmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c440790-8855-407f-d6db-988f641fab47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved the model as model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "j_Rr7mHb-xxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # batch size\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fa42ff-5f71-4273-c2b8-545265fb32de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.2509, Accuracy: 0.2188\n",
            "Step 100, Loss: 0.6063, Accuracy: 0.7469\n",
            "Step 200, Loss: 0.5280, Accuracy: 0.8094\n",
            "Step 300, Loss: 0.8186, Accuracy: 0.8371\n",
            "Step 400, Loss: 0.3434, Accuracy: 0.8569\n",
            "Step 500, Loss: 0.3929, Accuracy: 0.8692\n",
            "Step 600, Loss: 0.4328, Accuracy: 0.8773\n",
            "Step 700, Loss: 0.2887, Accuracy: 0.8830\n",
            "Step 800, Loss: 0.3304, Accuracy: 0.8890\n",
            "Step 900, Loss: 0.2511, Accuracy: 0.8935\n",
            "Step 1000, Loss: 0.8724, Accuracy: 0.8968\n",
            "Step 1100, Loss: 0.3645, Accuracy: 0.9001\n",
            "Step 1200, Loss: 0.3053, Accuracy: 0.9028\n",
            "Step 1300, Loss: 0.2077, Accuracy: 0.9053\n",
            "Step 1400, Loss: 0.9739, Accuracy: 0.9074\n",
            "Step 1500, Loss: 0.2332, Accuracy: 0.9098\n",
            "Step 1600, Loss: 0.2847, Accuracy: 0.9123\n",
            "Step 1700, Loss: 0.1629, Accuracy: 0.9145\n",
            "Step 1800, Loss: 0.1797, Accuracy: 0.9163\n",
            "Training Accuracy for epoch 1: 0.9176\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.2945, Accuracy: 0.8438\n",
            "Step 100, Loss: 0.3408, Accuracy: 0.9496\n",
            "Step 200, Loss: 0.2146, Accuracy: 0.9499\n",
            "Step 300, Loss: 0.2037, Accuracy: 0.9487\n",
            "Step 400, Loss: 0.1249, Accuracy: 0.9485\n",
            "Step 500, Loss: 0.2133, Accuracy: 0.9500\n",
            "Step 600, Loss: 0.2452, Accuracy: 0.9508\n",
            "Step 700, Loss: 0.1129, Accuracy: 0.9515\n",
            "Step 800, Loss: 0.0709, Accuracy: 0.9517\n",
            "Step 900, Loss: 0.0794, Accuracy: 0.9522\n",
            "Step 1000, Loss: 0.1385, Accuracy: 0.9529\n",
            "Step 1100, Loss: 0.2427, Accuracy: 0.9535\n",
            "Step 1200, Loss: 0.0627, Accuracy: 0.9536\n",
            "Step 1300, Loss: 0.1251, Accuracy: 0.9538\n",
            "Step 1400, Loss: 0.0357, Accuracy: 0.9540\n",
            "Step 1500, Loss: 0.3872, Accuracy: 0.9544\n",
            "Step 1600, Loss: 0.0490, Accuracy: 0.9553\n",
            "Step 1700, Loss: 0.0901, Accuracy: 0.9560\n",
            "Step 1800, Loss: 0.4594, Accuracy: 0.9564\n",
            "Training Accuracy for epoch 2: 0.9570\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.2532, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0452, Accuracy: 0.9709\n",
            "Step 200, Loss: 0.1791, Accuracy: 0.9672\n",
            "Step 300, Loss: 0.0315, Accuracy: 0.9669\n",
            "Step 400, Loss: 0.2577, Accuracy: 0.9676\n",
            "Step 500, Loss: 0.2252, Accuracy: 0.9666\n",
            "Step 600, Loss: 0.0744, Accuracy: 0.9665\n",
            "Step 700, Loss: 0.3233, Accuracy: 0.9662\n",
            "Step 800, Loss: 0.0168, Accuracy: 0.9664\n",
            "Step 900, Loss: 0.0236, Accuracy: 0.9669\n",
            "Step 1000, Loss: 0.0317, Accuracy: 0.9672\n",
            "Step 1100, Loss: 0.0339, Accuracy: 0.9673\n",
            "Step 1200, Loss: 0.0345, Accuracy: 0.9669\n",
            "Step 1300, Loss: 0.1427, Accuracy: 0.9670\n",
            "Step 1400, Loss: 0.0105, Accuracy: 0.9669\n",
            "Step 1500, Loss: 0.2021, Accuracy: 0.9672\n",
            "Step 1600, Loss: 0.1475, Accuracy: 0.9676\n",
            "Step 1700, Loss: 0.1030, Accuracy: 0.9681\n",
            "Step 1800, Loss: 0.0309, Accuracy: 0.9682\n",
            "Training Accuracy for epoch 3: 0.9683\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0295, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0199, Accuracy: 0.9734\n",
            "Step 200, Loss: 0.0200, Accuracy: 0.9750\n",
            "Step 300, Loss: 0.1286, Accuracy: 0.9749\n",
            "Step 400, Loss: 0.0652, Accuracy: 0.9746\n",
            "Step 500, Loss: 0.0627, Accuracy: 0.9752\n",
            "Step 600, Loss: 0.2681, Accuracy: 0.9755\n",
            "Step 700, Loss: 0.0482, Accuracy: 0.9753\n",
            "Step 800, Loss: 0.0099, Accuracy: 0.9751\n",
            "Step 900, Loss: 0.2473, Accuracy: 0.9751\n",
            "Step 1000, Loss: 0.0449, Accuracy: 0.9743\n",
            "Step 1100, Loss: 0.0411, Accuracy: 0.9745\n",
            "Step 1200, Loss: 0.1126, Accuracy: 0.9746\n",
            "Step 1300, Loss: 0.0738, Accuracy: 0.9744\n",
            "Step 1400, Loss: 0.0938, Accuracy: 0.9746\n",
            "Step 1500, Loss: 0.0544, Accuracy: 0.9750\n",
            "Step 1600, Loss: 0.0145, Accuracy: 0.9750\n",
            "Step 1700, Loss: 0.1027, Accuracy: 0.9750\n",
            "Step 1800, Loss: 0.4901, Accuracy: 0.9749\n",
            "Training Accuracy for epoch 4: 0.9750\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0228, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0559, Accuracy: 0.9842\n",
            "Step 200, Loss: 0.0019, Accuracy: 0.9824\n",
            "Step 300, Loss: 0.3399, Accuracy: 0.9824\n",
            "Step 400, Loss: 0.0542, Accuracy: 0.9815\n",
            "Step 500, Loss: 0.0687, Accuracy: 0.9813\n",
            "Step 600, Loss: 0.0325, Accuracy: 0.9811\n",
            "Step 700, Loss: 0.0383, Accuracy: 0.9807\n",
            "Step 800, Loss: 0.0181, Accuracy: 0.9805\n",
            "Step 900, Loss: 0.0134, Accuracy: 0.9806\n",
            "Step 1000, Loss: 0.0758, Accuracy: 0.9806\n",
            "Step 1100, Loss: 0.0507, Accuracy: 0.9804\n",
            "Step 1200, Loss: 0.0553, Accuracy: 0.9805\n",
            "Step 1300, Loss: 0.0132, Accuracy: 0.9803\n",
            "Step 1400, Loss: 0.0047, Accuracy: 0.9801\n",
            "Step 1500, Loss: 0.0393, Accuracy: 0.9802\n",
            "Step 1600, Loss: 0.0932, Accuracy: 0.9802\n",
            "Step 1700, Loss: 0.0025, Accuracy: 0.9802\n",
            "Step 1800, Loss: 0.0481, Accuracy: 0.9803\n",
            "Training Accuracy for epoch 5: 0.9801\n",
            "\n",
            "TF Training time: 371.70 seconds\n",
            "Test Accuracy: 0.9710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Optimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "OQqSrMDU-AlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53afd40-af46-4c2f-b3a7-605f0053d317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3858, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.7488, Accuracy: 0.7268\n",
            "Step 200, Loss: 0.3294, Accuracy: 0.8002\n",
            "Step 300, Loss: 0.5456, Accuracy: 0.8324\n",
            "Step 400, Loss: 0.1995, Accuracy: 0.8516\n",
            "Step 500, Loss: 0.4999, Accuracy: 0.8628\n",
            "Step 600, Loss: 0.2850, Accuracy: 0.8717\n",
            "Step 700, Loss: 0.2589, Accuracy: 0.8783\n",
            "Step 800, Loss: 0.2736, Accuracy: 0.8843\n",
            "Step 900, Loss: 0.2279, Accuracy: 0.8903\n",
            "Step 1000, Loss: 0.1297, Accuracy: 0.8945\n",
            "Step 1100, Loss: 0.4389, Accuracy: 0.8973\n",
            "Step 1200, Loss: 0.1601, Accuracy: 0.9000\n",
            "Step 1300, Loss: 0.2095, Accuracy: 0.9022\n",
            "Step 1400, Loss: 0.1394, Accuracy: 0.9049\n",
            "Step 1500, Loss: 0.2906, Accuracy: 0.9069\n",
            "Step 1600, Loss: 0.1251, Accuracy: 0.9093\n",
            "Step 1700, Loss: 0.2187, Accuracy: 0.9115\n",
            "Step 1800, Loss: 0.0425, Accuracy: 0.9139\n",
            "Training Accuracy for epoch 1: 0.9155\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0520, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1222, Accuracy: 0.9474\n",
            "Step 200, Loss: 0.0285, Accuracy: 0.9490\n",
            "Step 300, Loss: 0.3173, Accuracy: 0.9488\n",
            "Step 400, Loss: 0.3040, Accuracy: 0.9497\n",
            "Step 500, Loss: 0.2512, Accuracy: 0.9513\n",
            "Step 600, Loss: 0.0618, Accuracy: 0.9525\n",
            "Step 700, Loss: 0.0813, Accuracy: 0.9529\n",
            "Step 800, Loss: 0.2097, Accuracy: 0.9525\n",
            "Step 900, Loss: 0.1348, Accuracy: 0.9523\n",
            "Step 1000, Loss: 0.1106, Accuracy: 0.9532\n",
            "Step 1100, Loss: 0.1647, Accuracy: 0.9530\n",
            "Step 1200, Loss: 0.1715, Accuracy: 0.9529\n",
            "Step 1300, Loss: 0.1673, Accuracy: 0.9530\n",
            "Step 1400, Loss: 0.2424, Accuracy: 0.9536\n",
            "Step 1500, Loss: 0.1749, Accuracy: 0.9542\n",
            "Step 1600, Loss: 0.2548, Accuracy: 0.9547\n",
            "Step 1700, Loss: 0.1121, Accuracy: 0.9556\n",
            "Step 1800, Loss: 0.0654, Accuracy: 0.9561\n",
            "Training Accuracy for epoch 2: 0.9566\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1517, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0901, Accuracy: 0.9694\n",
            "Step 200, Loss: 0.0170, Accuracy: 0.9691\n",
            "Step 300, Loss: 0.0568, Accuracy: 0.9683\n",
            "Step 400, Loss: 0.1070, Accuracy: 0.9677\n",
            "Step 500, Loss: 0.0771, Accuracy: 0.9686\n",
            "Step 600, Loss: 0.2191, Accuracy: 0.9684\n",
            "Step 700, Loss: 0.0985, Accuracy: 0.9685\n",
            "Step 800, Loss: 0.0177, Accuracy: 0.9686\n",
            "Step 900, Loss: 0.0335, Accuracy: 0.9687\n",
            "Step 1000, Loss: 0.1581, Accuracy: 0.9688\n",
            "Step 1100, Loss: 0.0800, Accuracy: 0.9681\n",
            "Step 1200, Loss: 0.0820, Accuracy: 0.9682\n",
            "Step 1300, Loss: 0.0765, Accuracy: 0.9678\n",
            "Step 1400, Loss: 0.0180, Accuracy: 0.9679\n",
            "Step 1500, Loss: 0.1325, Accuracy: 0.9681\n",
            "Step 1600, Loss: 0.0370, Accuracy: 0.9683\n",
            "Step 1700, Loss: 0.0502, Accuracy: 0.9682\n",
            "Step 1800, Loss: 0.0352, Accuracy: 0.9685\n",
            "Training Accuracy for epoch 3: 0.9687\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1393, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1339, Accuracy: 0.9737\n",
            "Step 200, Loss: 0.0522, Accuracy: 0.9745\n",
            "Step 300, Loss: 0.0366, Accuracy: 0.9743\n",
            "Step 400, Loss: 0.0351, Accuracy: 0.9744\n",
            "Step 500, Loss: 0.0401, Accuracy: 0.9739\n",
            "Step 600, Loss: 0.0730, Accuracy: 0.9739\n",
            "Step 700, Loss: 0.0613, Accuracy: 0.9735\n",
            "Step 800, Loss: 0.0869, Accuracy: 0.9734\n",
            "Step 900, Loss: 0.0668, Accuracy: 0.9735\n",
            "Step 1000, Loss: 0.2045, Accuracy: 0.9738\n",
            "Step 1100, Loss: 0.0102, Accuracy: 0.9738\n",
            "Step 1200, Loss: 0.0256, Accuracy: 0.9741\n",
            "Step 1300, Loss: 0.0831, Accuracy: 0.9737\n",
            "Step 1400, Loss: 0.0539, Accuracy: 0.9737\n",
            "Step 1500, Loss: 0.1564, Accuracy: 0.9739\n",
            "Step 1600, Loss: 0.0345, Accuracy: 0.9743\n",
            "Step 1700, Loss: 0.0132, Accuracy: 0.9745\n",
            "Step 1800, Loss: 0.0245, Accuracy: 0.9747\n",
            "Training Accuracy for epoch 4: 0.9747\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.3628, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0945, Accuracy: 0.9796\n",
            "Step 200, Loss: 0.0399, Accuracy: 0.9806\n",
            "Step 300, Loss: 0.0100, Accuracy: 0.9797\n",
            "Step 400, Loss: 0.0851, Accuracy: 0.9790\n",
            "Step 500, Loss: 0.1942, Accuracy: 0.9789\n",
            "Step 600, Loss: 0.1278, Accuracy: 0.9790\n",
            "Step 700, Loss: 0.0456, Accuracy: 0.9790\n",
            "Step 800, Loss: 0.0613, Accuracy: 0.9787\n",
            "Step 900, Loss: 0.1479, Accuracy: 0.9785\n",
            "Step 1000, Loss: 0.0291, Accuracy: 0.9785\n",
            "Step 1100, Loss: 0.0658, Accuracy: 0.9779\n",
            "Step 1200, Loss: 0.0302, Accuracy: 0.9779\n",
            "Step 1300, Loss: 0.0100, Accuracy: 0.9779\n",
            "Step 1400, Loss: 0.0291, Accuracy: 0.9779\n",
            "Step 1500, Loss: 0.0079, Accuracy: 0.9782\n",
            "Step 1600, Loss: 0.1575, Accuracy: 0.9782\n",
            "Step 1700, Loss: 0.0731, Accuracy: 0.9782\n",
            "Step 1800, Loss: 0.1787, Accuracy: 0.9784\n",
            "Training Accuracy for epoch 5: 0.9786\n",
            "\n",
            "TF Training time: 36.27 seconds\n",
            "Test Accuracy: 0.9686\n"
          ]
        }
      ]
    }
  ]
}