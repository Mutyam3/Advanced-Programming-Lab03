{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJVqGvALPjXM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Use the MNIST dataset of handwritten digits (28x28 grayscale images, 10 classes).\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hDjFvIpQytI",
        "outputId": "10ac4992-f0cf-41f5-ace8-fa1f35961a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture\n",
        "# • Flatten input (784 features)\n",
        "# • Dense layer with 64 ReLU units\n",
        "# • Output layer with 10 units (softmax for TensorFlow)"
      ],
      "metadata": {
        "id": "ipLnFtf-ShlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),              # Input shape: 28x28 pixels\n",
        "    tf.keras.layers.Flatten(),                          # Flatten to 784 features\n",
        "    tf.keras.layers.Dense(64, activation='relu'),       # Hidden layer with 64 neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Output layer: 10 classes (digits 0–9)\n",
        "])\n"
      ],
      "metadata": {
        "id": "5XDmpgtrRWo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',   # Because labels are one-hot encoded\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"TF Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"accuracy : {test_accuracy * 100:.2f}%\")\n",
        "print(f\"loss: {test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi2ZVkYCScxu",
        "outputId": "b8c2ec41-9dfa-43ea-f2a0-29e822abb3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.4900\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9551 - loss: 0.1592\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1110\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0842\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.0699\n",
            "TF Training time: 24.41 seconds\n",
            "accuracy : 97.24%\n",
            "loss: 0.0930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time and get evaluation metrics\n",
        "start_inference = time.time()\n",
        "\n",
        "# model.evaluate runs inference on the test set and returns [loss, accuracy]\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "end_inference = time.time()\n",
        "inference_time = end_inference - start_inference\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Inference Time on Test Set: {inference_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnmPPVMWflkC",
        "outputId": "05d2fd00-6fee-4748-e5b3-4f0c576354e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1068\n",
            "Test Accuracy: 97.24%\n",
            "Inference Time on Test Set: 0.70 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LasW_2bsgm38",
        "outputId": "55fc1594-bf5b-47df-e511-f407951d33d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpxsc0q0l2'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135388343154640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135388343155600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135388343155024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135388343155408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch Implementation"
      ],
      "metadata": {
        "id": "U1dSQpQ7xAI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    # Fill correct input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    # Fill correct input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))    # Fill correct layer\n",
        "        return self.fc2(x)    # Fill correct layer\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajF0rmvxK_J",
        "outputId": "52644688-a9fc-4420-f0ab-9a896c121564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 63.81 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference and Evaluation using PyTorch’s model.eval() + torch.no_grad().\n",
        "model.eval()\n",
        "correct = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4u-kq-62AHx",
        "outputId": "107fd81f-c0cf-4fe4-cd47-29472c90d810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9647\n",
            "Inference Time (total): 1.1434 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg8B05z4zBoi",
        "outputId": "45c7f111-3ae1-4c6a-d9fc-d07cf1598dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ],
      "metadata": {
        "id": "8aM4mLT5zGmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "j_Rr7mHb-xxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Fill same batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7714fbcb-6f83-4cc7-af59-8dbf710929d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3117, Accuracy: 0.2500\n",
            "Step 100, Loss: 0.4687, Accuracy: 0.7259\n",
            "Step 200, Loss: 0.3867, Accuracy: 0.8036\n",
            "Step 300, Loss: 0.3799, Accuracy: 0.8339\n",
            "Step 400, Loss: 0.2365, Accuracy: 0.8542\n",
            "Step 500, Loss: 0.3407, Accuracy: 0.8643\n",
            "Step 600, Loss: 0.4211, Accuracy: 0.8728\n",
            "Step 700, Loss: 0.1327, Accuracy: 0.8795\n",
            "Step 800, Loss: 0.1089, Accuracy: 0.8848\n",
            "Step 900, Loss: 0.2745, Accuracy: 0.8904\n",
            "Step 1000, Loss: 0.1360, Accuracy: 0.8935\n",
            "Step 1100, Loss: 0.3220, Accuracy: 0.8969\n",
            "Step 1200, Loss: 0.3165, Accuracy: 0.9003\n",
            "Step 1300, Loss: 0.2738, Accuracy: 0.9034\n",
            "Step 1400, Loss: 0.2884, Accuracy: 0.9055\n",
            "Step 1500, Loss: 0.1636, Accuracy: 0.9081\n",
            "Step 1600, Loss: 0.0662, Accuracy: 0.9102\n",
            "Step 1700, Loss: 0.1036, Accuracy: 0.9128\n",
            "Step 1800, Loss: 0.1999, Accuracy: 0.9149\n",
            "Training Accuracy for epoch 1: 0.9164\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0633, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0747, Accuracy: 0.9570\n",
            "Step 200, Loss: 0.3089, Accuracy: 0.9520\n",
            "Step 300, Loss: 0.1288, Accuracy: 0.9522\n",
            "Step 400, Loss: 0.0472, Accuracy: 0.9536\n",
            "Step 500, Loss: 0.0971, Accuracy: 0.9540\n",
            "Step 600, Loss: 0.1524, Accuracy: 0.9540\n",
            "Step 700, Loss: 0.1053, Accuracy: 0.9548\n",
            "Step 800, Loss: 0.0702, Accuracy: 0.9548\n",
            "Step 900, Loss: 0.0953, Accuracy: 0.9552\n",
            "Step 1000, Loss: 0.1678, Accuracy: 0.9552\n",
            "Step 1100, Loss: 0.1728, Accuracy: 0.9559\n",
            "Step 1200, Loss: 0.1377, Accuracy: 0.9562\n",
            "Step 1300, Loss: 0.0826, Accuracy: 0.9562\n",
            "Step 1400, Loss: 0.0786, Accuracy: 0.9560\n",
            "Step 1500, Loss: 0.0534, Accuracy: 0.9560\n",
            "Step 1600, Loss: 0.1357, Accuracy: 0.9566\n",
            "Step 1700, Loss: 0.0220, Accuracy: 0.9575\n",
            "Step 1800, Loss: 0.4178, Accuracy: 0.9577\n",
            "Training Accuracy for epoch 2: 0.9582\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1438, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0849, Accuracy: 0.9660\n",
            "Step 200, Loss: 0.1855, Accuracy: 0.9661\n",
            "Step 300, Loss: 0.2416, Accuracy: 0.9656\n",
            "Step 400, Loss: 0.0733, Accuracy: 0.9652\n",
            "Step 500, Loss: 0.1528, Accuracy: 0.9658\n",
            "Step 600, Loss: 0.0328, Accuracy: 0.9667\n",
            "Step 700, Loss: 0.3259, Accuracy: 0.9667\n",
            "Step 800, Loss: 0.1114, Accuracy: 0.9667\n",
            "Step 900, Loss: 0.1308, Accuracy: 0.9675\n",
            "Step 1000, Loss: 0.2221, Accuracy: 0.9678\n",
            "Step 1100, Loss: 0.2298, Accuracy: 0.9684\n",
            "Step 1200, Loss: 0.2616, Accuracy: 0.9678\n",
            "Step 1300, Loss: 0.0102, Accuracy: 0.9677\n",
            "Step 1400, Loss: 0.0507, Accuracy: 0.9675\n",
            "Step 1500, Loss: 0.0345, Accuracy: 0.9681\n",
            "Step 1600, Loss: 0.1116, Accuracy: 0.9682\n",
            "Step 1700, Loss: 0.0284, Accuracy: 0.9681\n",
            "Step 1800, Loss: 0.0220, Accuracy: 0.9685\n",
            "Training Accuracy for epoch 3: 0.9686\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0948, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0323, Accuracy: 0.9762\n",
            "Step 200, Loss: 0.0292, Accuracy: 0.9776\n",
            "Step 300, Loss: 0.1551, Accuracy: 0.9745\n",
            "Step 400, Loss: 0.0142, Accuracy: 0.9750\n",
            "Step 500, Loss: 0.0854, Accuracy: 0.9757\n",
            "Step 600, Loss: 0.0238, Accuracy: 0.9748\n",
            "Step 700, Loss: 0.0803, Accuracy: 0.9748\n",
            "Step 800, Loss: 0.0216, Accuracy: 0.9746\n",
            "Step 900, Loss: 0.0611, Accuracy: 0.9746\n",
            "Step 1000, Loss: 0.0885, Accuracy: 0.9746\n",
            "Step 1100, Loss: 0.0606, Accuracy: 0.9746\n",
            "Step 1200, Loss: 0.0608, Accuracy: 0.9746\n",
            "Step 1300, Loss: 0.1252, Accuracy: 0.9741\n",
            "Step 1400, Loss: 0.0989, Accuracy: 0.9744\n",
            "Step 1500, Loss: 0.0152, Accuracy: 0.9746\n",
            "Step 1600, Loss: 0.1162, Accuracy: 0.9750\n",
            "Step 1700, Loss: 0.1277, Accuracy: 0.9752\n",
            "Step 1800, Loss: 0.0847, Accuracy: 0.9750\n",
            "Training Accuracy for epoch 4: 0.9751\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0323, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0278, Accuracy: 0.9780\n",
            "Step 200, Loss: 0.0337, Accuracy: 0.9806\n",
            "Step 300, Loss: 0.1539, Accuracy: 0.9805\n",
            "Step 400, Loss: 0.1205, Accuracy: 0.9806\n",
            "Step 500, Loss: 0.0714, Accuracy: 0.9799\n",
            "Step 600, Loss: 0.0220, Accuracy: 0.9792\n",
            "Step 700, Loss: 0.0155, Accuracy: 0.9794\n",
            "Step 800, Loss: 0.0601, Accuracy: 0.9793\n",
            "Step 900, Loss: 0.0748, Accuracy: 0.9797\n",
            "Step 1000, Loss: 0.1319, Accuracy: 0.9794\n",
            "Step 1100, Loss: 0.0667, Accuracy: 0.9793\n",
            "Step 1200, Loss: 0.0295, Accuracy: 0.9787\n",
            "Step 1300, Loss: 0.1360, Accuracy: 0.9788\n",
            "Step 1400, Loss: 0.0192, Accuracy: 0.9789\n",
            "Step 1500, Loss: 0.0321, Accuracy: 0.9790\n",
            "Step 1600, Loss: 0.0398, Accuracy: 0.9793\n",
            "Step 1700, Loss: 0.1379, Accuracy: 0.9791\n",
            "Step 1800, Loss: 0.0853, Accuracy: 0.9788\n",
            "Training Accuracy for epoch 5: 0.9790\n",
            "\n",
            "TF Training time: 391.66 seconds\n",
            "Test Accuracy: 0.9727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Optimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "OQqSrMDU-AlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2edf869-bc30-4133-c4c7-47c9b02cb3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3543, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.4293, Accuracy: 0.7299\n",
            "Step 200, Loss: 0.2289, Accuracy: 0.8032\n",
            "Step 300, Loss: 0.2192, Accuracy: 0.8380\n",
            "Step 400, Loss: 0.4730, Accuracy: 0.8562\n",
            "Step 500, Loss: 0.2580, Accuracy: 0.8665\n",
            "Step 600, Loss: 0.2297, Accuracy: 0.8758\n",
            "Step 700, Loss: 0.2165, Accuracy: 0.8827\n",
            "Step 800, Loss: 0.2771, Accuracy: 0.8883\n",
            "Step 900, Loss: 0.2225, Accuracy: 0.8929\n",
            "Step 1000, Loss: 0.3704, Accuracy: 0.8965\n",
            "Step 1100, Loss: 0.4104, Accuracy: 0.8988\n",
            "Step 1200, Loss: 0.1558, Accuracy: 0.9014\n",
            "Step 1300, Loss: 0.0976, Accuracy: 0.9036\n",
            "Step 1400, Loss: 0.0955, Accuracy: 0.9062\n",
            "Step 1500, Loss: 0.5124, Accuracy: 0.9087\n",
            "Step 1600, Loss: 0.0296, Accuracy: 0.9110\n",
            "Step 1700, Loss: 0.3256, Accuracy: 0.9132\n",
            "Step 1800, Loss: 0.0991, Accuracy: 0.9152\n",
            "Training Accuracy for epoch 1: 0.9164\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1072, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0315, Accuracy: 0.9511\n",
            "Step 200, Loss: 0.3384, Accuracy: 0.9541\n",
            "Step 300, Loss: 0.1710, Accuracy: 0.9518\n",
            "Step 400, Loss: 0.0918, Accuracy: 0.9525\n",
            "Step 500, Loss: 0.0667, Accuracy: 0.9530\n",
            "Step 600, Loss: 0.1380, Accuracy: 0.9544\n",
            "Step 700, Loss: 0.0840, Accuracy: 0.9548\n",
            "Step 800, Loss: 0.2712, Accuracy: 0.9553\n",
            "Step 900, Loss: 0.2228, Accuracy: 0.9554\n",
            "Step 1000, Loss: 0.0500, Accuracy: 0.9558\n",
            "Step 1100, Loss: 0.0877, Accuracy: 0.9559\n",
            "Step 1200, Loss: 0.1364, Accuracy: 0.9559\n",
            "Step 1300, Loss: 0.1861, Accuracy: 0.9560\n",
            "Step 1400, Loss: 0.1044, Accuracy: 0.9561\n",
            "Step 1500, Loss: 0.0644, Accuracy: 0.9564\n",
            "Step 1600, Loss: 0.1741, Accuracy: 0.9568\n",
            "Step 1700, Loss: 0.1099, Accuracy: 0.9570\n",
            "Step 1800, Loss: 0.0767, Accuracy: 0.9577\n",
            "Training Accuracy for epoch 2: 0.9582\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0698, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.3183, Accuracy: 0.9700\n",
            "Step 200, Loss: 0.1320, Accuracy: 0.9726\n",
            "Step 300, Loss: 0.0513, Accuracy: 0.9707\n",
            "Step 400, Loss: 0.0557, Accuracy: 0.9674\n",
            "Step 500, Loss: 0.0223, Accuracy: 0.9678\n",
            "Step 600, Loss: 0.1485, Accuracy: 0.9687\n",
            "Step 700, Loss: 0.0686, Accuracy: 0.9683\n",
            "Step 800, Loss: 0.0794, Accuracy: 0.9680\n",
            "Step 900, Loss: 0.1234, Accuracy: 0.9678\n",
            "Step 1000, Loss: 0.1521, Accuracy: 0.9679\n",
            "Step 1100, Loss: 0.0734, Accuracy: 0.9678\n",
            "Step 1200, Loss: 0.0619, Accuracy: 0.9675\n",
            "Step 1300, Loss: 0.0831, Accuracy: 0.9677\n",
            "Step 1400, Loss: 0.0313, Accuracy: 0.9674\n",
            "Step 1500, Loss: 0.1759, Accuracy: 0.9677\n",
            "Step 1600, Loss: 0.0125, Accuracy: 0.9679\n",
            "Step 1700, Loss: 0.0466, Accuracy: 0.9683\n",
            "Step 1800, Loss: 0.2754, Accuracy: 0.9687\n",
            "Training Accuracy for epoch 3: 0.9690\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0712, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0122, Accuracy: 0.9783\n",
            "Step 200, Loss: 0.0168, Accuracy: 0.9765\n",
            "Step 300, Loss: 0.0338, Accuracy: 0.9763\n",
            "Step 400, Loss: 0.1460, Accuracy: 0.9765\n",
            "Step 500, Loss: 0.1448, Accuracy: 0.9765\n",
            "Step 600, Loss: 0.1395, Accuracy: 0.9756\n",
            "Step 700, Loss: 0.0646, Accuracy: 0.9750\n",
            "Step 800, Loss: 0.0130, Accuracy: 0.9754\n",
            "Step 900, Loss: 0.0665, Accuracy: 0.9751\n",
            "Step 1000, Loss: 0.0144, Accuracy: 0.9751\n",
            "Step 1100, Loss: 0.0117, Accuracy: 0.9751\n",
            "Step 1200, Loss: 0.0079, Accuracy: 0.9750\n",
            "Step 1300, Loss: 0.1467, Accuracy: 0.9749\n",
            "Step 1400, Loss: 0.0456, Accuracy: 0.9751\n",
            "Step 1500, Loss: 0.0526, Accuracy: 0.9750\n",
            "Step 1600, Loss: 0.0537, Accuracy: 0.9749\n",
            "Step 1700, Loss: 0.2188, Accuracy: 0.9751\n",
            "Step 1800, Loss: 0.0441, Accuracy: 0.9753\n",
            "Training Accuracy for epoch 4: 0.9754\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0111, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0136, Accuracy: 0.9821\n",
            "Step 200, Loss: 0.0476, Accuracy: 0.9803\n",
            "Step 300, Loss: 0.1030, Accuracy: 0.9790\n",
            "Step 400, Loss: 0.1222, Accuracy: 0.9797\n",
            "Step 500, Loss: 0.0371, Accuracy: 0.9805\n",
            "Step 600, Loss: 0.0411, Accuracy: 0.9800\n",
            "Step 700, Loss: 0.0427, Accuracy: 0.9794\n",
            "Step 800, Loss: 0.0435, Accuracy: 0.9796\n",
            "Step 900, Loss: 0.0368, Accuracy: 0.9797\n",
            "Step 1000, Loss: 0.0224, Accuracy: 0.9799\n",
            "Step 1100, Loss: 0.0402, Accuracy: 0.9794\n",
            "Step 1200, Loss: 0.0712, Accuracy: 0.9789\n",
            "Step 1300, Loss: 0.0570, Accuracy: 0.9789\n",
            "Step 1400, Loss: 0.0251, Accuracy: 0.9787\n",
            "Step 1500, Loss: 0.0508, Accuracy: 0.9788\n",
            "Step 1600, Loss: 0.0302, Accuracy: 0.9790\n",
            "Step 1700, Loss: 0.0512, Accuracy: 0.9792\n",
            "Step 1800, Loss: 0.0101, Accuracy: 0.9792\n",
            "Training Accuracy for epoch 5: 0.9793\n",
            "\n",
            "TF Training time: 23.34 seconds\n",
            "Test Accuracy: 0.9729\n"
          ]
        }
      ]
    }
  ]
}